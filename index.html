
<title>Meng Yu | Audio & Speech</title>

<center><table align="center" width="1053"><tr><td>
<img align="left" src="me.png" width="145">

<h2>Meng Yu</h2> 


<p>I'm a Principle Research Scientist at <a href="https://ai.tencent.com/ailab/" target="_blank">Tencent AI Lab</a> since 2016. From 2013 to 2016, I worked as a staff research engineer on audio/speech enhancement for voice communication and improved speech recognition in Audience (a Knowles Company). I was a software engineer working on speaker segmentation and recognition in Cisco from 2012 to 2013. I received my B.S. in Mathematics from Peking University, Beijing, China in 2007, 
and a Ph.D. degree in Mathematics from University of California, Irvine, CA, USA in 2012. </p>

<p>My research interests are in audio/speech processing and related fields, particularly in single and multi-channel speech enhancement, 
dereverberation, echo cancellation and other far field frontend speech enhancement applications; 
deep learning based speech enhancement and separation (cocktail party problem); 
and their joint optimization with keyword spotting and acoustic modeling of speech recognition. </p>

<p>I serve as a reviewer for ICASSP, Interspeech, IEEE Workshop on Application of Signal Processing to Audio and Acoustics (WASPAA), 
IEEE Digital Signal Processing Workshop, IEEE /ACM Transactions on Audio, Speech and Language Processing, 
EURASIP Journal on Audio, Speech, and Music Processing, Journal of Computer Science and Technology, ACM Multimedia, Speech communication, Communications in Mathematical Sciences, etc.

<p>Email: yumeng.ssym AT gmail.com</p>

<p>Linkedin: <a href="https://www.linkedin.com/in/menyu" target="_blank">Click here</a></p>


<h3>Publications</h3>
<p>[34] <embed src="C:\Users\raymondmyu\Documents\GitHub\raymond-myu.github.io\ICASSP_KWS.pdf" />Integration of Multi-Look Beamformers for Multi-Channel Keyword Spotting</a>, X. Ji, <b>M. Yu</b>, J. Chen, J. Zheng, D. Su and D. Yu, submitted to ICASSP, 2020</p>

<p>[33] <a href="https://cmsworkshops.com/ASRU2019/Papers/ViewPaper.asp?PaperNum=1059" target="_blank">Improving Speech Enhancement with Phonetic Embedding Features</a>, B. Wu, <b>M. Yu</b>, L. Chen, M. Jin, D. Su, D. Yu, accepted to ASRU, 2019</p>

<p>[32] <a href="https://cmsworkshops.com/ASRU2019/Papers/ViewPaper.asp?PaperNum=1165" target="_blank">Syllable-Dependent Discriminative learning for Small Footprint Text-Dependent Speaker Verification</a>, J. Peng, N. Li, D. Tuo, <b>M. Yu</b>, C. Zhang, Y. Zou, D. Su and D. Yu, accepted to ASRU, 2019</p>

<p>[31] <a href="https://arxiv.org/pdf/1910.13825.pdf" target="_blank">Overlapped speech recognition from a jointly learned multi-channel neural speech extraction and representation</a>, B. Wu, <b>M. Yu</b>, L. Chen, C. Weng, D. Su and D. Yu, arXiv:1910.13825, 2019</p>

<p>[30] <a href="https://arxiv.org/pdf/1909.01700.pdf" target="_blank">DurIAN: Duration Informed Attention Network For Multimodal Synthesis</a>, C. Yu, H. Lu, N. Hu <b>M. Yu</b>, C. Weng, K. Xu, P. Liu, D. Tuo, S. Kang, G. Lei, D. Su and D. Yu, arXiv preprint arXiv:1909.01700, 2019</p>

<p>[29] <a href="https://arxiv.org/pdf/1909.07352.pdf" target="_blank"> Audio-Visual Speech Separation and Dereverberation with a Two-Stage Multimodal Network</a>, K. Tan, Y. Xu, S. Zhang, <b>M. Yu</b>, and D. Yu, arXiv preprint arXiv:1909.07352, 2019</p>

<p>[28] <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1242.pdf" target="_blank">Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition</a>, B. Liu, S. Nie, S. Liang, W. Liu, <b>M. Yu</b>, L. Chen, S. Peng and C. Li, Interspeech, 2019</p>

<p>[27] <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1474.pdf" target="_blank"> Direction-aware Speaker Beam for Multi-channel Speaker Extraction</a>, G. Li, S. Liang, S. Nie, W. Liu, <b>M. Yu</b>, L. Chen, S. Peng and C. Li, Interspeech, 2019</p>

<p>[26] <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2266.pdf" target="_blank"> Neural Spatial Filter: Target Speaker Speech Separation Assisted withDirectional Information</a>, R. Gu, L. Chen, S. Zhang, J. Zheng, Y. Xu, <b>M. Yu</b>, D. Su, Y. Zou, D. Yu, Interspeech, 2019</p>

<p>[25] <a href="https://arxiv.org/pdf/1905.07497.pdf" target="_blank">A comprehensive study of speech separation: spectrogram vs waveform
separation</a>, F. Bahmaninezhad, J. Wu, R. Gu, S. Zhang, Y. Xu, <b>M. Yu</b>, and D. Yu, arXiv:1905.07497, Interspeech, 2019</p>

<p>[24] <a href="https://arxiv.org/pdf/1905.06286" target="_blank">End-to-End Multi-Channel Speech Separation</a>, 
R. Gu, J. Wu, S. Zhang, L. Chen, Y. Xu, <b>M. Yu</b>, D. Su, Y. Zou, D. Yu, arXiv:1905.06286, 2019</p>

<p>[23] <a href="https://arxiv.org/abs/1904.03760" target="_blank">Time Domain Audio Visual Speech Separation</a>, 
J. Wu, Y. Xu, S. Zhang, L. Chen, <b>M. Yu</b>, L. Xie, D. Yu, arXiv:1904.03760, accepted to ASRU, 2019</p>

<p>[22] <a href="https://arxiv.org/abs/1904.03792" target="_blank">Improved Speaker-Dependent Separation for CHiME-5 Challenge</a>, 
J. Wu, Y. Xu, S. Zhang, L. Chen, <b>M. Yu</b>, L. Xie, D. Yu, arXiv:1904.03792, Interspeech, 2019</p>

<p>[21] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682749" target="_blank">Boundary Discriminative Large Margin Cosine Loss for Text-independent Speaker Verification</a>,
R. Li, N. Li, D. Tuo, <b>M. Yu</b>, D. Su, and D. Yu, ICASSP, 2019</p>

<p>[20] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682576" target="_blank">Joint Training of Complex Ratio Mask Based Beamformer and Acoustic Model for Noise Robust Asr</a>,
Y. Xu, C. Weng, L. Hui, J. Liu, <b>M. Yu</b>, D. Su and D. Yu, ICASSP, 2019</p>

<p>[19] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682676" target="_blank">Seq2Seq Attentional Siamese Neural Networks for Text-dependent Speaker Verification</a>,
Y. Zhang, <b>M. Yu</b>, N. Li, C. Yu, J. Cui and D. Yu, ICASSP, 2019</p>

<p>[18] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682470" target="_blank">Multi-band PIT and Model Integration for Improved Multi-channel Speech Separation</a>, 
L. Chen, <b>M. Yu</b>, Y. Qian, D. Su, and D. Yu, ICASSP, 2019</p>

<p>[17] <a href="https://arxiv.org/pdf/1807.08974.pdf" target="_blank">Deep extractor network for target speaker recovery from single channel speech mixtures</a>, 
J. Wang, J. Chen, D. Su, L. Chen, <b>M. Yu</b>, Y. Qian, D. Yu, Interspeech, 2018</p>

<p>[16] <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1668.pdf" target="_blank">Text-Dependent Speech Enhancement for Small-Footprint Robust Keyword Detection</a>, 
<b>M. Yu</b>, X. Ji, Y. Gao, L. Chen, J. Chen, J. Zheng, D. Su, and D. Yu, Interspeech, 2018</p>

<p>[15] <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1603.pdf" target="_blank">Permutation Invariant Training of Generative Adversarial Network for Monaural Speech Separation</a>, 
L. Chen, <b>M. Yu</b>, Y. Qian, D. Su, and D. Yu, Interspeech, 2018</p>

<p>[14] <a href="https://patents.google.com/patent/US9048942B2/en" target="_blank">Method and system for reducing interference and noise in speech signals</a>, 
J. Hershey and <b>M. Yu</b>, 2015</p>

<p>[13] <a href="https://reverb2014.dereverberation.com/workshop/reverb2014-papers/1569898953.pdf" target="_blank">Speech Dereverberation by Constrained and Regularized Multi-Channel Spectral Decomposition</a>, 
<b>M. Yu</b> and F. K. Soong, REVERB Challenge workshop, 2014</p>

<p>[12] <a href="https://www.math.uci.edu/~jxin/IEEE_CSE_final_double.pdf" target="_blank">Multi-channel L1 regularized convex speech enhancement model and fast computation by the split Bregman method</a>, 
<b>M. Yu</b>, W. Ma, J. Xin and S. Osher, IEEE Tran. on Audio, Speech and Language Proc., 20(2), pp. 661-675, 2012</p>

<p>[11] <a href="https://www.intlpress.com/site/pub/files/_fulltext/journals/cms/2012/0010/0001/CMS-2012-0010-0001-a011.pdf" target="_blank">A convex model and L1 minimization for musical noise reduction in blind source separation</a>, 
<b>M. Yu</b>, W. Ma, J. Xin and S. Osher, Communications in Mathematical Sciences, 10(1), pp 223-238, 2012</p>

<p>[10] <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2012/i12_1938.pdf" target="_blank">Constrained multichannel speech dereverberation</a>, 
<b>M. Yu</b> and F. K. Soong, Interspeech, 2012</p>

<p>[09] <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2012/i12_0150.pdf" target="_blank">Exploring off-time nature for speech enhancement</a>, 
<b>M. Yu</b> and J. Xin, Interspeech, 2012</p>

<p>[08] <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2012/i12_1942.pdf" target="_blank">A triple-microphone real-time speech enhancement algorithm based on approximate array analytical solutions</a>, 
<b>M. Yu</b>, R. Ritch, and J. Xin, Interspeech, 2012</p>

<p>[07] <a href="http://iccm2012.com/proceedings/papers/0018/paper0018.pdf" target="_blank">Exploring feature collocation for semantic concept identification</a>, 
<b>M. Yu</b>, ICCM, 2012</p>

<p>[06] <a href="http://www.socsci.uci.edu/~mdlee/ZhangEtAl2011.pdf" target="_blank">Identification of semantic categories: a sparse representation approach</a>, 
S. Zhang, <b>M. Yu</b>, M. Lee and J. Xin, CogSci, 2011 </p>

<p>[05] <a href="https://www.researchgate.net/profile/Meng_Yu10/publication/228647380_Stochastic_approximation_and_a_nonlocally_weighted_soft-constrained_recursive_algorithm_for_blind_separation_of_reverberant_speech_mixtures/links/0f31753c6b8c7ce9c3000000.pdf" target="_blank">Stochastic approximation and a nonlocally weighted soft-constrained recursive algorithm for blind separation of reverberant speech mixtures</a>, 
<b>M. Yu</b> and J. Xin, Discrete and Continuous Dynamical Systems, vo. 28, no. 4, 2010 </p>

<p>[04] <a href="https://yifeizhu.github.io/bkos.pdf" target="_blank"> Convexity and fast speech extraction by split Bregman method</a>, 
<b>M. Yu</b>, W. Ma, J. Xin and S. Osher, Interspeech, 2010</p>

<p>[03] <a href="https://www.math.uci.edu/~jxin/interspeech_10_MYXO.pdf" target="_blank">Reducing musical noise in blind source separation by time-domain sparse filters and split Bregman method</a>, 
<b>M. Yu</b>, W. Ma, J. Xin and S. Osher, Interspeech, 2010</p>

<p>[02] <a href="https://ieeexplore.ieee.org/document/5470080" target="_blank">A nonlocally weighted soft-constrained natural gradient algorithm and blind separation of strong reverberant speech mixtures</a>, 
<b>M. Yu</b>, J. Xin, Y. Qi, H. Yang and F-G Zeng, 43rd Asilomar Conference on Signals, Systems, and Computers, pp. 346-350, 2009</p>

<p>[01] <a href="https://www.math.uci.edu/~jxin/waspaa09_XYQYZ.pdf" target="_blank">A nonlocally weighted soft-constrained natural gradient algorithm for blind separation of reverberation speech</a>, 
<b>M. Yu</b>, J. Xin, Y. Qi, H. Yang and F-G Zeng, WASPAA, pp. 81 - 84, 2009</p>




<p></p>

</td></tr></table></center>
