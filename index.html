
<title>Meng Yu | Audio & Speech</title>

<center><table align="center" width="1053"><tr><td>
<img align="left" src="me.png" width="145">

<h2>Meng Yu</h2> 


<p>I'm a Principle Research Scientist at <a href="https://ai.tencent.com/ailab/" target="_blank">Tencent AI Lab</a> since 2016. From 2013 to 2016, I worked as a staff research engineer on audio/speech enhancement for voice communication and improved speech recognition in Audience (a Knowles Company). I was a software engineer working on speaker segmentation and recognition in Cisco from 2012 to 2013. I received my B.S. in Mathematics from Peking University, Beijing, China in 2007, 
and a Ph.D. degree in Mathematics from University of California, Irvine, CA, USA in 2012. </p>

<p>My research interests are in audio/speech processing and related fields, particularly in single and multi-channel speech enhancement, 
dereverberation, echo cancellation and other far field frontend speech enhancement applications; 
deep learning based speech enhancement and separation (cocktail party problem); 
and their joint optimization with keyword spotting, speaker recognition and acoustic modeling of speech recognition. </p>

<p>I serve as a reviewer for ICASSP, Interspeech, IEEE Workshop on Application of Signal Processing to Audio and Acoustics (WASPAA), 
IEEE Digital Signal Processing Workshop, IEEE /ACM Transactions on Audio, Speech and Language Processing, 
EURASIP Journal on Audio, Speech, and Music Processing, Journal of Computer Science and Technology, ACM Multimedia, Speech communication, Communications in Mathematical Sciences, etc.

<p>Email: mraymondyu AT gmail.com</p>

<p>Linkedin: <a href="https://www.linkedin.com/in/menyu" target="_blank">Click here</a></p>
<p>CV: <a href="CV2021.pdf" target="_blank">Click here</a></p>
<p>Google Scholar: <a href="https://scholar.google.com/citations?user=pxFJcEEAAAAJ&hl=en" target="_blank">Click here</a></p>


<h3>Selected Publications</h3>
<p>[17] <a href="https://arxiv.org/pdf/2104.01227.pdf" target="_blank">MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment</a>, 
<b>M Yu</b>, C Zhang, Y Xu, S Zhang, D Yu, arXiv preprint arXiv:2104.01227</p>

<p>[16] <a href="https://arxiv.org/pdf/2103.08781.pdf" target="_blank">Towards Robust Speaker Verification with Target Speaker Enhancement</a>, C Zhang, <b>M Yu</b>, C Weng, D Yu, ICASSP, 2021</p>

<p>[15] <a href="https://arxiv.org/pdf/2008.06994.pdf" target="_blank">ADL-MVDR: All deep learning MVDR beamformer for target speech separation</a>, Z Zhang, Y Xu, <b>M Yu</b>, SX Zhang, L Chen, D Yu, ICASSP, 2021</p>

<p>[14] <a href="https://arxiv.org/pdf/2008.09586.pdf" target="_blank">An overview of deep-learning-based audio-visual speech enhancement and separation</a>,
D Michelsanti, ZH Tan, SX Zhang, Y Xu, <b>M Yu</b>, D Yu, J Jensen, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021</p>

<p>[13] <a href="https://cmsworkshops.com/ICASSP2020/Papers/AbstractSearch.asp?show=search" target="_blank">Speaker-Aware Target Speaker Enhancement by Jointly Learning with Speaker Embedding Extraction</a>, X. Ji, <b>M. Yu</b>, C. Zhang, D. Su, T. Yu, X. Liu, and D. Yu, ICASSP, 2020</p>

<p>[12] <a href="https://arxiv.org/pdf/2005.10386.pdf" target="_blank">End-to-End Multi-Look Keyword Spotting</a>, <b>M Yu</b>, X Ji, B Wu, D Su, D Yu, Interspeech 2020</p>

<p>[11] <a href="https://cmsworkshops.com/ICASSP2020/Papers/AbstractSearch.asp?show=search" target="_blank">Integration of Multi-Look Beamformers for Multi-Channel Keyword Spotting</a>, X. Ji, <b>M. Yu</b>, J. Chen, J. Zheng, D. Su and D. Yu, ICASSP, 2020</p>

<p>[10] <a href="https://arxiv.org/pdf/1912.07814.pdf" target="_blank">A Unified Framework for Speech Separation</a>, 
F Bahmaninezhad, SX Zhang, Y Xu, <b>M Yu</b>, JHL Hansen, D Yu, arXiv preprint arXiv:1912.07814</p>

<p>[09] <a href="https://arxiv.org/pdf/1904.03760.pdf" target="_blank">Time domain audio visual speech separation</a>, 
J. Wu, Y Xu, SX Zhang, LW Chen, <b>M. Yu</b>, L. Xie, D. Yu, ASRU, 2019</p>

<p>[08] <a href="https://arxiv.org/pdf/1910.13825.pdf" target="_blank">Overlapped speech recognition from a jointly learned multi-channel neural speech extraction and representation</a>, B. Wu, <b>M. Yu</b>, L. Chen, C. Weng, D. Su and D. Yu, arXiv:1910.13825, 2019</p>

<p>[07] <a href="https://arxiv.org/pdf/1909.01700.pdf" target="_blank">DurIAN: Duration Informed Attention Network For Multimodal Synthesis</a>, C. Yu, H. Lu, N. Hu <b>M. Yu</b>, C. Weng, K. Xu, P. Liu, D. Tuo, S. Kang, G. Lei, D. Su and D. Yu, arXiv preprint arXiv:1909.01700, 2019</p>

<p>[06] <a href="https://arxiv.org/pdf/1905.07497.pdf" target="_blank">A comprehensive study of speech separation: spectrogram vs waveform
separation</a>, F. Bahmaninezhad, J. Wu, R. Gu, S. Zhang, Y. Xu, <b>M. Yu</b>, and D. Yu, arXiv:1905.07497, Interspeech, 2019</p>

<p>[05] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682676" target="_blank">Seq2Seq Attentional Siamese Neural Networks for Text-dependent Speaker Verification</a>,
Y. Zhang, <b>M. Yu</b>, N. Li, C. Yu, J. Cui and D. Yu, ICASSP, 2019</p>

<p>[04] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682470" target="_blank">Multi-band PIT and Model Integration for Improved Multi-channel Speech Separation</a>, 
L. Chen, <b>M. Yu</b>, Y. Qian, D. Su, and D. Yu, ICASSP, 2019</p>

<p>[03] <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1668.pdf" target="_blank">Text-Dependent Speech Enhancement for Small-Footprint Robust Keyword Detection</a>, 
<b>M. Yu</b>, X. Ji, Y. Gao, L. Chen, J. Chen, J. Zheng, D. Su, and D. Yu, Interspeech, 2018</p>

<p>[02] <a href="https://reverb2014.dereverberation.com/workshop/reverb2014-papers/1569898953.pdf" target="_blank">Speech Dereverberation by Constrained and Regularized Multi-Channel Spectral Decomposition</a>, 
<b>M. Yu</b> and F. K. Soong, REVERB Challenge workshop, 2014</p>

<p>[01] <a href="https://www.math.uci.edu/~jxin/IEEE_CSE_final_double.pdf" target="_blank">Multi-channel L1 regularized convex speech enhancement model and fast computation by the split Bregman method</a>, 
<b>M. Yu</b>, W. Ma, J. Xin and S. Osher, IEEE Tran. on Audio, Speech and Language Proc., 20(2), pp. 661-675, 2012</p>







<p></p>

</td></tr></table></center>
